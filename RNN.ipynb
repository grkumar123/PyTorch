{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYofH3eb3rmdgvnhysF3G6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**https://www.youtube.com/watch?v=Gl2WXLIMvKA&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=5**"],"metadata":{"id":"UsAK8yQNZ7dB"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"YBEd-KyJZ3zc","executionInfo":{"status":"ok","timestamp":1665465688539,"user_tz":-330,"elapsed":1511,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn   # All neural network modules nn.Linear, nn.conv2d, BatchNorm, Loss functions\n","import torch.optim as optim  # For all optimizations algorithms SGD, Adams etc.\n","import torch.nn.functional as F # All functions that don't have any parameters\n","from torch.utils.data import DataLoader # Gives easier dataset management and creates mini batches\n","import torchvision.datasets as datasets  # Has standard dataset we can import in a nice way\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset"]},{"cell_type":"code","source":["# Create RNN\n","class RNN(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","    super(RNN, self).__init__()\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n","    self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n","\n","  def forward(self, x):\n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","    # Forward prop\n","    out, _ = self.rnn(x, h0)\n","    out = out.reshape(out.shape[0], -1)\n","    out = self.fc(out)\n","    return out\n","\n","# Set Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 28\n","sequence_length = 28\n","num_layers = 2\n","hidden_size = 256\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 2\n","\n","# Load Data\n","train_dataset = datasets.MNIST('/content/', train=True, transform=transforms.ToTensor(), download=True)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataset = datasets.MNIST('/content/', train=False, transform=transforms.ToTensor(), download=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize Network\n","model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","  for batch_idx,(data, targets) in enumerate(train_loader):\n","    # Get data to cuda if possible\n","    data = data.to(device=device).squeeze(1)\n","    targets = targets.to(device=device)\n","\n","    # Forward\n","    scores = model(data)\n","    loss = criterion(scores, targets)\n","\n","    # Backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Gradient Descent or Adam Step\n","    optimizer.step()\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","  if loader.dataset.train:\n","    print(\"Checking accuracy on training data\")\n","  else:\n","    print(\"Checking accuracy on test data\")\n","  \n","  num_correct = 0\n","  num_samples = 0\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for x, y in loader:\n","      x = x.to(device=device).squeeze(1)\n","      y = y.to(device=device)\n","\n","      scores = model(x)\n","      _, predictions = scores.max(1)\n","      num_correct += (predictions == y).sum()\n","      num_samples += predictions.size(0)\n","\n","    #print(f'Got (num_correct)/(num_samples) with accuracy (float(num_correct)/float(num_samples)*100:.2f)')\n","    print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n","\n","  model.train()"],"metadata":{"id":"GcESCdUyZ412","executionInfo":{"status":"ok","timestamp":1665465819021,"user_tz":-330,"elapsed":14705,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["check_accuracy(train_loader, model)\n","check_accuracy(test_loader, model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhz6LopRZ439","executionInfo":{"status":"ok","timestamp":1665465825075,"user_tz":-330,"elapsed":6108,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}},"outputId":"2fe558d8-4551-4532-9a94-a2461acaad50"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking accuracy on training data\n","Got 57735/60000 with accuracy 96.23\n","Checking accuracy on test data\n","Got 9631/10000 with accuracy 96.31\n"]}]}]}