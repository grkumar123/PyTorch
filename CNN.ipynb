{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNggX0o+Nu8U3vEyh3BoaeX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**https://www.youtube.com/watch?v=wnK3uWv_WkU&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=4**"],"metadata":{"id":"xvwHnSrhPBEg"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"SxKG4UhuO3nz","executionInfo":{"status":"ok","timestamp":1665397711545,"user_tz":-330,"elapsed":21,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn   # All neural network modules nn.Linear, nn.conv2d, BatchNorm, Loss functions\n","import torch.optim as optim  # For all optimizations algorithms SGD, Adams etc.\n","import torch.nn.functional as F # All functions that don't have any parameters\n","from torch.utils.data import DataLoader # Gives easier dataset management and creates mini batches\n","import torchvision.datasets as datasets  # Has standard dataset we can import in a nice way\n","import torchvision.transforms as transforms  # Transformations we can perform on our dataset"]},{"cell_type":"code","source":["# Create CNN Module\n","class CNN(nn.Module):\n","  def __init__(self, in_channels=1, num_classes=10):\n","    super(CNN, self).__init__()\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","    self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n","    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","    self.fc1 = nn.Linear(16*7*7, num_classes)\n","\n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = self.pool(x)\n","    x = F.relu(self.conv2(x))\n","    x = self.pool(x)\n","    x = x.reshape(x.shape[0], -1)\n","    x = self.fc1(x)\n","    return x\n","\n","# Set Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","input_size = 784\n","num_classes = 10\n","learning_rate = 0.001\n","batch_size = 64\n","num_epochs = 5\n","\n","# Load Data\n","train_dataset = datasets.MNIST('/content/', train=True, transform=transforms.ToTensor(), download=True)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataset = datasets.MNIST('/content/', train=False, transform=transforms.ToTensor(), download=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize Network\n","model = CNN().to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train Network\n","for epoch in range(num_epochs):\n","  for batch_idx,(data, targets) in enumerate(train_loader):\n","    # Get data to cuda if possible\n","    data = data.to(device=device)\n","    targets = targets.to(device=device)\n","\n","    # Forward\n","    scores = model(data)\n","    loss = criterion(scores, targets)\n","\n","    # Backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Gradient Descent or Adam Step\n","    optimizer.step()\n","\n","# Check accuracy on training & test to see how good our model\n","def check_accuracy(loader, model):\n","  if loader.dataset.train:\n","    print(\"Checking accuracy on training data\")\n","  else:\n","    print(\"Checking accuracy on test data\")\n","  \n","  num_correct = 0\n","  num_samples = 0\n","  model.eval()\n","\n","  with torch.no_grad():\n","    for x, y in loader:\n","      x = x.to(device=device)\n","      y = y.to(device=device)\n","\n","      scores = model(x)\n","      _, predictions = scores.max(1)\n","      num_correct += (predictions == y).sum()\n","      num_samples += predictions.size(0)\n","\n","    #print(f'Got (num_correct)/(num_samples) with accuracy (float(num_correct)/float(num_samples)*100:.2f)')\n","    print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n","\n","  model.train()"],"metadata":{"id":"LjPn7_Y9PMe9","executionInfo":{"status":"ok","timestamp":1665397813087,"user_tz":-330,"elapsed":33800,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["check_accuracy(train_loader, model)\n","check_accuracy(test_loader, model)"],"metadata":{"id":"50wXAVxqPMh7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665397818002,"user_tz":-330,"elapsed":4926,"user":{"displayName":"Gyan Kumar","userId":"04208958827417762560"}},"outputId":"55dcad08-e0fe-487c-932d-e2f810d38e82"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking accuracy on training data\n","Got 59189/60000 with accuracy 98.65\n","Checking accuracy on test data\n","Got 9849/10000 with accuracy 98.49\n"]}]}]}